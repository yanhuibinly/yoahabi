# LLM Environment variables

# Reasoning LLM (for complex reasoning tasks)
# If you're using your local Ollama, replace the model name after the slash and base url then you're good to go.
# For wider model support, read https://docs.litellm.ai/docs/providers.
REASONING_API_KEY=sk-3b86c3748c864860bab509a3560aee11
REASONING_BASE_URL=https://api.deepseek.com
REASONING_MODEL=deepseek-reasoner
# REASONING_API_KEY=
# REASONING_BASE_URL=http://localhost:11434
# REASONING_MODEL=ollama/qwen3:14b

# Non-reasoning LLM (for straightforward tasks)
BASIC_API_KEY=sk-42f98e806f7a4c25809937c3c1fd3b60
BASIC_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
BASIC_MODEL=qwen-max
# BASIC_API_KEY=
# BASIC_BASE_URL=http://localhost:11434
# BASIC_MODEL=ollama/gemma3:4b

# Vision-language LLM (for tasks requiring visual understanding)
VL_API_KEY=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
VL_BASE_URL=https://api.openai.com/v1/
VL_MODEL=gpt-4o
# VL_API_KEY=
# VL_BASE_URL=http://localhost:11434
# VL_MODEL=ollama/qwen2.5vl:latest


# Application Settings
DEBUG=True
APP_ENV=development

# Add other environment variables as needed
TAVILY_API_KEY=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
JINA_API_KEY=jina_ed446871e276423681aea383d8a387d0s2cVisr9GWN-_XDLOqGHFReICdl1 # Optional, default is None

# CHROME_INSTANCE_PATH=/Applications/Google Chrome.app/Contents/MacOS/Google Chrome
# CHROME_HEADLESS=False  # Optional, default is False
# CHROME_PROXY_SERVER=http://127.0.0.1:10809  # Optional, default is None
# CHROME_PROXY_USERNAME=  # Optional, default is None
# CHROME_PROXY_PASSWORD=  # Optional, default is None

LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT="https://api.smith.langchain.com"
LANGSMITH_API_KEY=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


